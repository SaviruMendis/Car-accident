{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde47400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: kaggle in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (1.8.3)\n",
      "Requirement already satisfied: kagglesdk<1.0,>=0.1.14 in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from kagglehub) (0.1.14)\n",
      "Requirement already satisfied: packaging in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from kagglehub) (25.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from kagglehub) (6.0.3)\n",
      "Requirement already satisfied: requests in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from kagglehub) (2.32.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from kagglesdk<1.0,>=0.1.14->kagglehub) (6.33.4)\n",
      "Requirement already satisfied: black>=24.10.0 in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from kaggle) (25.12.0)\n",
      "Requirement already satisfied: bleach in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from kaggle) (6.3.0)\n",
      "Requirement already satisfied: mypy>=1.15.0 in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from kaggle) (1.19.1)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from kaggle) (1.17.0)\n",
      "Requirement already satisfied: types-requests in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from kaggle) (2.32.4.20260107)\n",
      "Requirement already satisfied: types-tqdm in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from kaggle) (4.67.0.20250809)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from kaggle) (2.6.3)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from black>=24.10.0->kaggle) (8.3.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from black>=24.10.0->kaggle) (1.1.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from black>=24.10.0->kaggle) (1.0.3)\n",
      "Requirement already satisfied: platformdirs>=2 in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from black>=24.10.0->kaggle) (4.5.1)\n",
      "Requirement already satisfied: pytokens>=0.3.0 in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from black>=24.10.0->kaggle) (0.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from click>=8.0.0->black>=24.10.0->kaggle) (0.4.6)\n",
      "Requirement already satisfied: typing_extensions>=4.6.0 in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from mypy>=1.15.0->kaggle) (4.15.0)\n",
      "Requirement already satisfied: librt>=0.6.2 in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from mypy>=1.15.0->kaggle) (0.7.7)\n",
      "Requirement already satisfied: webencodings in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from requests->kagglehub) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from requests->kagglehub) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saviru mendis\\desktop\\statistical learning 1\\data analysis project i\\venv\\lib\\site-packages (from requests->kagglehub) (2026.1.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Saviru Mendis\\Desktop\\Statistical learning 1\\Data Analysis Project I\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#If you don't have kagglehub, install it with pip install kagglehub\n",
    "#also install, pip install kaggle\n",
    "%pip install kagglehub kaggle\n",
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"ianktoo/simulated-roads-accident-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7924ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f639f0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "df1 = pd.read_csv(os.path.join(path, \"synthetic_road_accidents_2k.csv\"))\n",
    "df2 = pd.read_csv(os.path.join(path, \"synthetic_road_accidents_10k.csv\"))\n",
    "df3 = pd.read_csv(os.path.join(path, \"synthetic_road_accidents_100k.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a694071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112000, 13)\n"
     ]
    }
   ],
   "source": [
    "_df = pd.concat([df1, df2, df3])\n",
    "print(_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62eaee63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates after removal: 0\n",
      "Shape of the df after removal: (111961, 13)\n"
     ]
    }
   ],
   "source": [
    "#Final combined dataframe\n",
    "_df = _df.drop_duplicates()\n",
    "print(f\"Number of duplicates after removal: {_df.duplicated().sum()}\")\n",
    "print(f\"Shape of the df after removal: {_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "999ec233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 111961 entries, 0 to 111960\n",
      "Data columns (total 13 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   road_type               111961 non-null  object \n",
      " 1   num_lanes               111961 non-null  int64  \n",
      " 2   curvature               111961 non-null  float64\n",
      " 3   speed_limit             111961 non-null  int64  \n",
      " 4   lighting                111961 non-null  object \n",
      " 5   weather                 111961 non-null  object \n",
      " 6   road_signs_present      111961 non-null  bool   \n",
      " 7   public_road             111961 non-null  bool   \n",
      " 8   time_of_day             111961 non-null  object \n",
      " 9   holiday                 111961 non-null  bool   \n",
      " 10  school_season           111961 non-null  bool   \n",
      " 11  num_reported_accidents  111961 non-null  int64  \n",
      " 12  accident_risk           111961 non-null  float64\n",
      "dtypes: bool(4), float64(2), int64(3), object(4)\n",
      "memory usage: 8.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#final_df with corrected column indexes.\n",
    "_df.reset_index(drop = True, inplace = True)\n",
    "print(_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e966e89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_copy = _df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1948c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = _df_copy.drop([\"accident_risk\", \"num_reported_accidents\"], axis=1)\n",
    "y = _df_copy[\"accident_risk\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9699984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78372, 12)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "f_df = pd.concat([X_train, y_train], axis = 1)\n",
    "print(f_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd9596fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kmodes.kprototypes import KPrototypes\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21e14cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical: ['road_type', 'lighting', 'weather', 'road_signs_present', 'public_road', 'time_of_day', 'holiday', 'school_season']\n",
      "Numerical: ['num_lanes', 'curvature', 'speed_limit']\n"
     ]
    }
   ],
   "source": [
    "# separate features (no target)\n",
    "X_full = _df_copy.drop([\"accident_risk\", \"num_reported_accidents\"], axis=1)\n",
    "\n",
    "# detect categorical columns\n",
    "categorical_cols = X_full.select_dtypes(include=[\"object\", \"bool\"]).columns.tolist()\n",
    "numerical_cols = X_full.select_dtypes(exclude=[\"object\", \"bool\"]).columns.tolist()\n",
    "\n",
    "print(\"Categorical:\", categorical_cols)\n",
    "print(\"Numerical:\", numerical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f569b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = X_full.copy()\n",
    "\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_encoded[col] = le.fit_transform(X_full[col])\n",
    "    label_encoders[col] = le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eda81919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization method and algorithm are deterministic. Setting n_init to 1.\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run: 1, iteration: 1/100, moves: 9680, ncost: 2748581.566375378\n",
      "Run: 1, iteration: 2/100, moves: 0, ncost: 2748581.566375378\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run: 2, iteration: 1/100, moves: 1937, ncost: 4307340.134535434\n",
      "Run: 2, iteration: 2/100, moves: 0, ncost: 4307340.134535434\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run: 3, iteration: 1/100, moves: 6130, ncost: 2748581.566375378\n",
      "Run: 3, iteration: 2/100, moves: 0, ncost: 2748581.566375378\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run: 4, iteration: 1/100, moves: 3528, ncost: 2750330.0221200916\n",
      "Run: 4, iteration: 2/100, moves: 0, ncost: 2750330.0221200916\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run: 5, iteration: 1/100, moves: 13610, ncost: 2748581.566375378\n",
      "Run: 5, iteration: 2/100, moves: 0, ncost: 2748581.566375378\n",
      "Best run was number 1\n"
     ]
    }
   ],
   "source": [
    "# get categorical column indices\n",
    "cat_idx = [X_encoded.columns.get_loc(col) for col in categorical_cols]\n",
    "\n",
    "X_train_enc = X_encoded.loc[X_train.index]\n",
    "X_test_enc  = X_encoded.loc[X_test.index]\n",
    "\n",
    "kproto = KPrototypes(\n",
    "    n_clusters=3,\n",
    "    init=\"Cao\",\n",
    "    n_init=5,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "train_clusters = kproto.fit_predict(\n",
    "    X_train_enc.to_numpy(),\n",
    "    categorical=cat_idx\n",
    ")\n",
    "\n",
    "test_clusters = kproto.predict(\n",
    "    X_test_enc.to_numpy(),\n",
    "    categorical=cat_idx\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01134c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clustered = X_train.copy()\n",
    "X_test_clustered  = X_test.copy()\n",
    "\n",
    "\n",
    "X_train_clustered[\"cluster\"] = train_clusters\n",
    "X_test_clustered[\"cluster\"]  = test_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50e65b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster\n",
       "0    0.302638\n",
       "1    0.503213\n",
       "2    0.302463\n",
       "Name: accident_risk, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_clustered = pd.concat(\n",
    "    [X_train_clustered, y_train], axis=1\n",
    ")\n",
    "\n",
    "train_df_clustered.groupby(\"cluster\")[\"accident_risk\"].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cb154c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71e66d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical features for CatBoost\n",
    "cat_features_cb = [\n",
    "    X_train_clustered.columns.get_loc(col)\n",
    "    for col in categorical_cols\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "198237d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R² without cluster: 0.8737510120017573\n",
      "Train R² without cluster: 0.8754838431225889\n"
     ]
    }
   ],
   "source": [
    "model_no_cluster = CatBoostRegressor(\n",
    "    iterations=600,\n",
    "    depth=6,\n",
    "    learning_rate=0.05,\n",
    "    loss_function=\"RMSE\",\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "model_no_cluster.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cat_features=cat_features_cb\n",
    ")\n",
    "\n",
    "y_pred_nc = model_no_cluster.predict(X_test)\n",
    "y_train_pred_nc = model_no_cluster.predict(X_train)\n",
    "\n",
    "print(\"Test R² without cluster:\", r2_score(y_test, y_pred_nc))\n",
    "print(\"Train R² without cluster:\", r2_score(y_train, y_train_pred_nc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13c9ea58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Test No Clusters): 0.0633\n",
      "RMSE (Train No Clusters): 0.0633\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Predictions\n",
    "y_pred_no_cluster = model_no_cluster.predict(X_test)\n",
    "\n",
    "# RMSE\n",
    "rmse_no_cluster = np.sqrt(mean_squared_error(y_test, y_pred_no_cluster))\n",
    "rmse_train_nc = np.sqrt(mean_squared_error(y_train, y_train_pred_nc))\n",
    "\n",
    "print(f\"RMSE (Test No Clusters): {rmse_no_cluster:.4f}\")\n",
    "print(f\"RMSE (Train No Clusters): {rmse_train_nc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6860d63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 | Train size = 15654\n",
      "Cluster 1 | Train size = 31491\n",
      "Cluster 2 | Train size = 31227\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "cluster_models = {}\n",
    "\n",
    "for c in sorted(train_df_clustered['cluster'].unique()):\n",
    "    \n",
    "    sub_df = train_df_clustered[train_df_clustered['cluster'] == c]\n",
    "\n",
    "    X_c = sub_df.drop(\"accident_risk\", axis=1)\n",
    "    y_c = sub_df[\"accident_risk\"]\n",
    "\n",
    "    model_c = CatBoostRegressor(\n",
    "        iterations=400,\n",
    "        depth=5,\n",
    "        learning_rate=0.05,\n",
    "        loss_function=\"RMSE\",\n",
    "        random_state=42,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    model_c.fit(\n",
    "        X_c,\n",
    "        y_c,\n",
    "        cat_features=cat_features_cb\n",
    "    )\n",
    "\n",
    "    cluster_models[c] = model_c\n",
    "\n",
    "    print(f\"Cluster {c} | Train size = {len(X_c)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01b76d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_clustered = pd.concat(\n",
    "    [X_test_clustered, y_test], axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4f31f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 | Test R² = 0.8234 | n = 6817\n",
      "Cluster 1 | Test R² = 0.8190 | n = 13434\n",
      "Cluster 2 | Test R² = 0.8191 | n = 13338\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_per_cluster = {}\n",
    "\n",
    "for c, model in cluster_models.items():\n",
    "    \n",
    "    test_sub = test_df_clustered[test_df_clustered[\"cluster\"] == c]\n",
    "\n",
    "    # skip if too small (important for stability)\n",
    "    if len(test_sub) < 50:\n",
    "        print(f\"Cluster {c}: too few samples ({len(test_sub)})\")\n",
    "        continue\n",
    "\n",
    "    X_c_test = test_sub.drop(\"accident_risk\", axis=1)\n",
    "    y_c_test = test_sub[\"accident_risk\"]\n",
    "\n",
    "    y_pred_c = model.predict(X_c_test)\n",
    "\n",
    "    r2 = r2_score(y_c_test, y_pred_c)\n",
    "    r2_per_cluster[c] = r2\n",
    "\n",
    "    print(f\"Cluster {c} | Test R² = {r2:.4f} | n = {len(test_sub)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4380e2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 RMSE: 0.0634\n",
      "Cluster 1 RMSE: 0.0637\n",
      "Cluster 2 RMSE: 0.0628\n"
     ]
    }
   ],
   "source": [
    "for cluster_id in sorted(test_df_clustered[\"cluster\"].unique()):\n",
    "    \n",
    "    test_sub = test_df_clustered[test_df_clustered[\"cluster\"] == cluster_id]\n",
    "    \n",
    "    X_c_test = test_sub.drop(\"accident_risk\", axis=1)\n",
    "    y_c_test = test_sub[\"accident_risk\"]\n",
    "    \n",
    "    y_pred_c = cluster_models[cluster_id].predict(X_c_test)\n",
    "    \n",
    "    rmse_c = np.sqrt(mean_squared_error(y_c_test, y_pred_c))\n",
    "    \n",
    "    print(f\"Cluster {cluster_id} RMSE: {rmse_c:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00dd3354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted R² (cluster models): 0.8199341492687116\n"
     ]
    }
   ],
   "source": [
    "total_n = sum(len(test_df_clustered[test_df_clustered[\"cluster\"] == c])\n",
    "              for c in r2_per_cluster)\n",
    "\n",
    "weighted_r2 = sum(\n",
    "    r2_per_cluster[c] * len(test_df_clustered[test_df_clustered[\"cluster\"] == c])\n",
    "    for c in r2_per_cluster\n",
    ") / total_n\n",
    "\n",
    "print(\"Weighted R² (cluster models):\", weighted_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb06e5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CLUSTERED MODEL – TRAIN RESULTS (INDIVIDUAL)\n",
      "\n",
      "Cluster 0\n",
      "Samples   : 15654\n",
      "Train RMSE: 0.0625\n",
      "Train R²  : 0.8240\n",
      "\n",
      "Cluster 1\n",
      "Samples   : 31491\n",
      "Train RMSE: 0.0639\n",
      "Train R²  : 0.8215\n",
      "\n",
      "Cluster 2\n",
      "Samples   : 31227\n",
      "Train RMSE: 0.0632\n",
      "Train R²  : 0.8211\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCLUSTERED MODEL – TRAIN RESULTS (INDIVIDUAL)\")\n",
    "\n",
    "for c, model in cluster_models.items():\n",
    "    sub = train_df_clustered[train_df_clustered[\"cluster\"] == c]\n",
    "\n",
    "    X_c = sub.drop(\"accident_risk\", axis=1)\n",
    "    y_c = sub[\"accident_risk\"]\n",
    "\n",
    "    y_pred = model.predict(X_c)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_c, y_pred))\n",
    "    r2 = r2_score(y_c, y_pred)\n",
    "\n",
    "    print(f\"\\nCluster {c}\")\n",
    "    print(f\"Samples   : {len(sub)}\")\n",
    "    print(f\"Train RMSE: {rmse:.4f}\")\n",
    "    print(f\"Train R²  : {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9aacf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CLUSTERED MODEL – TEST RESULTS (INDIVIDUAL)\n",
      "\n",
      "Cluster 0\n",
      "Samples  : 6817\n",
      "Test RMSE: 0.0634\n",
      "Test R²  : 0.8234\n",
      "\n",
      "Cluster 1\n",
      "Samples  : 13434\n",
      "Test RMSE: 0.0637\n",
      "Test R²  : 0.8190\n",
      "\n",
      "Cluster 2\n",
      "Samples  : 13338\n",
      "Test RMSE: 0.0628\n",
      "Test R²  : 0.8191\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCLUSTERED MODEL – TEST RESULTS (INDIVIDUAL)\")\n",
    "\n",
    "for c, model in cluster_models.items():\n",
    "    sub = test_df_clustered[test_df_clustered[\"cluster\"] == c]\n",
    "\n",
    "    if len(sub) < 30:\n",
    "        print(f\"\\nCluster {c} skipped (too few samples: {len(sub)})\")\n",
    "        continue\n",
    "\n",
    "    X_c = sub.drop(\"accident_risk\", axis=1)\n",
    "    y_c = sub[\"accident_risk\"]\n",
    "\n",
    "    y_pred = model.predict(X_c)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_c, y_pred))\n",
    "    r2 = r2_score(y_c, y_pred)\n",
    "\n",
    "    print(f\"\\nCluster {c}\")\n",
    "    print(f\"Samples  : {len(sub)}\")\n",
    "    print(f\"Test RMSE: {rmse:.4f}\")\n",
    "    print(f\"Test R²  : {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0fe32cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_train_c = []\n",
    "r2_train_c = []\n",
    "weights_train = []\n",
    "\n",
    "for c, model in cluster_models.items():\n",
    "    sub = train_df_clustered[train_df_clustered[\"cluster\"] == c]\n",
    "    X_c = sub.drop(\"accident_risk\", axis=1)\n",
    "    y_c = sub[\"accident_risk\"]\n",
    "    \n",
    "    y_pred = model.predict(X_c)\n",
    "    \n",
    "    rmse_train_c.append(\n",
    "        np.sqrt(mean_squared_error(y_c, y_pred)) * len(sub)\n",
    "    )\n",
    "    r2_train_c.append(\n",
    "        r2_score(y_c, y_pred) * len(sub)\n",
    "    )\n",
    "    weights_train.append(len(sub))\n",
    "\n",
    "rmse_train_clustered = sum(rmse_train_c) / sum(weights_train)\n",
    "r2_train_clustered   = sum(r2_train_c) / sum(weights_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29c06ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_test_c = []\n",
    "r2_test_c = []\n",
    "weights_test = []\n",
    "\n",
    "for c, model in cluster_models.items():\n",
    "    sub = test_df_clustered[test_df_clustered[\"cluster\"] == c]\n",
    "    \n",
    "    if len(sub) < 30:\n",
    "        continue\n",
    "    \n",
    "    X_c = sub.drop(\"accident_risk\", axis=1)\n",
    "    y_c = sub[\"accident_risk\"]\n",
    "    \n",
    "    y_pred = model.predict(X_c)\n",
    "    \n",
    "    rmse_test_c.append(\n",
    "        np.sqrt(mean_squared_error(y_c, y_pred)) * len(sub)\n",
    "    )\n",
    "    r2_test_c.append(\n",
    "        r2_score(y_c, y_pred) * len(sub)\n",
    "    )\n",
    "    weights_test.append(len(sub))\n",
    "\n",
    "rmse_test_clustered = sum(rmse_test_c) / sum(weights_test)\n",
    "r2_test_clustered   = sum(r2_test_c) / sum(weights_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eab35a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CLUSTERED MODEL\n",
      "Train RMSE: 0.0633\n",
      "Train R²  : 0.8218\n",
      "Test RMSE : 0.0633\n",
      "Test R²   : 0.8199\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCLUSTERED MODEL\")\n",
    "print(f\"Train RMSE: {rmse_train_clustered:.4f}\")\n",
    "print(f\"Train R²  : {r2_train_clustered:.4f}\")\n",
    "print(f\"Test RMSE : {rmse_test_clustered:.4f}\")\n",
    "print(f\"Test R²   : {r2_test_clustered:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
